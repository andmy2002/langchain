{
 "cells": [
  {
   "cell_type": "raw",
   "id": "df29b30a-fd27-4e08-8269-870df5631f9e",
   "metadata": {},
   "source": [
    "---\n",
    "title: Extracting structured output\n",
    "sidebar_class_name: hidden\n",
    "---\n",
    "标题：提取结构化输出\n",
    "sidebar_class_name：隐藏\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e397959-1622-4c1c-bdb6-4660a3c39e14",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Large Language Models (LLMs) are emerging as an extremely capable technology for powering information extraction applications.\n",
    "\n",
    "Classical solutions to information extraction rely on a combination of people, (many) hand-crafted rules (e.g., regular expressions), and custom fine-tuned ML models.\n",
    "\n",
    "Such systems tend to get complex over time and become progressively more expensive to maintain and more difficult to enhance.\n",
    "\n",
    "LLMs can be adapted quickly for specific extraction tasks just by providing appropriate instructions to them and appropriate reference examples.\n",
    "\n",
    "This guide will show you how to use LLMs for extraction applications!\n",
    "\n",
    "## Approaches\n",
    "\n",
    "There are 3 broad approaches for information extraction using LLMs:\n",
    "\n",
    "- **Tool/Function Calling** Mode: Some LLMs support a *tool or function calling* mode. These LLMs can structure output according to a given **schema**. Generally, this approach is the easiest to work with and is expected to yield good results.\n",
    "\n",
    "- **JSON Mode**: Some LLMs are can be forced to output valid JSON. This is similar to **tool/function Calling** approach, except that the schema is provided as part of the prompt. Generally, our intuition is that this performs worse than a **tool/function calling** approach, but don't trust us and verify for your own use case!\n",
    "\n",
    "- **Prompting Based**: LLMs that can follow instructions well can be instructed to generate text in a desired format. The generated text can be parsed downstream using existing [Output Parsers](/docs/modules/model_io/output_parsers/) or using [custom parsers](/docs/modules/model_io/output_parsers/custom) into a structured format like JSON. This approach can be used with LLMs that **do not support** JSON mode or tool/function calling modes. This approach is more broadly applicable, though may yield worse results than models that have been fine-tuned for extraction or function calling.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "Head to the [quickstart](/docs/use_cases/extraction/quickstart) to see how to extract information using LLMs using a basic end-to-end example.\n",
    "\n",
    "The quickstart focuses on information extraction using the **tool/function calling** approach.\n",
    "\n",
    "\n",
    "## How-To Guides\n",
    "\n",
    "- [Use Reference Examples](/docs/use_cases/extraction/how_to/examples): Learn how to use **reference examples** to improve performance.\n",
    "- [Handle Long Text](/docs/use_cases/extraction/how_to/handle_long_text): What should you do if the text does not fit into the context window of the LLM?\n",
    "- [Handle Files](/docs/use_cases/extraction/how_to/handle_files): Examples of using LangChain document loaders and parsers to extract from files like PDFs.\n",
    "- [Use a Parsing Approach](/docs/use_cases/extraction/how_to/parse): Use a prompt based approach to extract with models that do not support **tool/function calling**.\n",
    "\n",
    "## Guidelines\n",
    "\n",
    "Head to the [Guidelines](/docs/use_cases/extraction/guidelines) page to see a list of opinionated guidelines on how to get the best performance for extraction use cases.\n",
    "\n",
    "## Use Case Accelerant\n",
    "\n",
    "[langchain-extract](https://github.com/langchain-ai/langchain-extract) is a starter repo that implements a simple web server for information extraction from text and files using LLMs. It is build using **FastAPI**, **LangChain** and **Postgresql**. Feel free to adapt it to your own use cases.\n",
    "\n",
    "## Other Resources\n",
    "\n",
    "* The [output parser](/docs/modules/model_io/output_parsers/) documentation includes various parser examples for specific types (e.g., lists, datetime, enum, etc).\n",
    "* LangChain [document loaders](/docs/modules/data_connection/document_loaders/) to load content from files. Please see list of [integrations](/docs/integrations/document_loaders).\n",
    "* The experimental [Anthropic function calling](/docs/integrations/chat/anthropic_functions) support provides similar functionality to Anthropic chat models.\n",
    "* [LlamaCPP](/docs/integrations/llms/llamacpp#grammars) natively supports constrained decoding using custom grammars, making it easy to output structured content using local LLMs \n",
    "* [JSONFormer](/docs/integrations/llms/jsonformer_experimental) offers another way for structured decoding of a subset of the JSON Schema.\n",
    "* [Kor](https://eyurtsev.github.io/kor/) is another library for extraction where schema and examples can be provided to the LLM. Kor is optimized to work for a parsing approach.\n",
    "* [OpenAI's function and tool calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "* For example, see [OpenAI's JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode).\n",
    "\n",
    "<hr>\n",
    "\n",
    "## 概述\n",
    "\n",
    "大型语言模型 (LLM) 正在成为一种为信息提取应用程序提供支持的极其强大的技术。\n",
    "\n",
    "信息提取的经典解决方案依赖于人员、（许多）手工制定的规则（例如正则表达式）和自定义微调的 ML 模型的组合。\n",
    "\n",
    "随着时间的推移，此类系统往往会变得复杂，维护成本也越来越高，增强起来也越来越困难。\n",
    "\n",
    "只需向法学硕士提供适当的说明和适当的参考示例，即可快速适应特定的提取任务。\n",
    "\n",
    "本指南将向您展示如何使用法学硕士进行提取应用程序！\n",
    "\n",
    "## 方法\n",
    "\n",
    "使用LLMs进行信息提取有 3 种广泛的方法：\n",
    "\n",
    "- **工具/函数调用**模式：一些LLMs支持*工具或函数调用*模式。 这些LLMs可以根据给定的**模式**构建输出。 一般来说，这种方法最容易使用，并且有望产生良好的结果。\n",
    "\n",
    "- **JSON 模式**：某些LLMs可以强制输出有效的 JSON。 这类似于 **工具/函数调用** 方法，只不过架构是作为提示的一部分提供的。 一般来说，我们的直觉是，这比 **工具/函数调用** 方法的性能更差，但不要相信我们并针对您自己的用例进行验证！\n",
    "\n",
    "- **基于提示**：可以很好地遵循说明LLMs可以被指示生成所需格式的文本。 生成的文本可以使用现有的[输出解析器](/docs/modules/model_io/output_parsers/)或使用[自定义解析器](/docs/modules/model_io/output_parsers/custom)在下游解析为结构化格式，如JSON。 此方法可用于 **不支持** JSON 模式或工具/函数调用模式的 LLM。 这种方法具有更广泛的适用性，但可能会比针对提取或函数调用进行微调的模型产生更差的结果。\n",
    "\n",
    "＃＃ 快速开始\n",
    "\n",
    "前往[快速入门](/docs/use_cases/extraction/quickstart)，了解如何使用基本的端到端示例使用 LLM 提取信息。\n",
    "\n",
    "本快速入门重点介绍使用**工具/函数调用**方法提取信息。\n",
    "\n",
    "\n",
    "## 操作指南\n",
    "\n",
    "- [使用参考示例](/docs/use_cases/extraction/how_to/examples)：了解如何使用**参考示例**来提高性能。\n",
    "- [处理长文本](/docs/use_cases/extraction/how_to/handle_long_text)：如果文本不适合法学硕士的上下文窗口，你该怎么办？\n",
    "- [处理文件](/docs/use_cases/extraction/how_to/handle_files)：使用 LangChain 文档加载器和解析器从 PDF 等文件中提取的示例。\n",
    "- [使用解析方法](/docs/use_cases/extraction/how_to/parse)：使用基于提示的方法对不支持**工具/函数调用**的模型进行提取。\n",
    "\n",
    "## 指南\n",
    "\n",
    "前往 [指南](/docs/use_cases/extraction/guidelines) 页面查看有关如何获得提取用例最佳性能的固执己见的指南列表。\n",
    "\n",
    "## 用例加速器\n",
    "\n",
    "[langchain-extract](https://github.com/langchain-ai/langchain-extract) 是一个入门存储库，它实现了一个简单的 Web 服务器，用于使用 LLM 从文本和文件中提取信息。 它是使用 **FastAPI**、**LangChain** 和 **Postgresql** 构建的。 请随意调整它以适应您自己的用例。\n",
    "\n",
    "## 其他资源\n",
    "\n",
    "* [输出解析器](/docs/modules/model_io/output_parsers/) 文档包含特定类型（例如列表、日期时间、枚举等）的各种解析器示例。\n",
    "* LangChain [文档加载器](/docs/modules/data_connection/document_loaders/) 从文件加载内容。 请参阅[集成](/docs/integrations/document_loaders) 列表。\n",
    "* 实验性的[人择函数调用](/docs/integrations/chat/anthropic_functions) 支持提供与人择聊天模型类似的功能。\n",
    "* [LlamaCPP](/docs/integrations/llms/llamacpp#grammars) 原生支持使用自定义语法的约束解码，从而可以轻松使用本地 LLM 输出结构化内容\n",
    "* [JSONFormer](/docs/integrations/llms/jsonformer_experimental) 提供了另一种对 JSON 模式子集进行结构化解码的方法。\n",
    "* [Kor](https://eyurtsev.github.io/kor/) 是另一个用于提取的库，可以向法学硕士提供模式和示例。 Kor 已针对解析方法进行了优化。\n",
    "* [OpenAI的函数和工具调用](https://platform.openai.com/docs/guides/function-calling)\n",
    "* 例如，请参阅[OpenAI的JSON模式](https://platform.openai.com/docs/guides/text- Generation/json-mode)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
